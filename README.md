# Involuntary-Jailbreak

Implementing this method using APIs is quite simple and can be done in just a few steps. 
In this version, we simply share the `meta_prompt` in `prompt_template/meta_prompts.py` for responsible reasons. 

## Disclaimer
The jailbreaking methods described in this paper are solely intended for responsible AI safety research and the development of more robust, secure AI systems.

### We aim to:
- Help researchers and developers identify vulnerabilities and strengthen defenses.
- Enable the AI community to proactively mitigate risks before malicious exploitation.
- Foster responsible innovation in AI security.

### We urge all readers and users to:
- Any misuse for harmful, unethical, or unauthorized purposes is strictly prohibited.
- Adhere to ethical guidelines and respect platform terms of service.
- Use this knowledge responsibly to advance safe and beneficial AI technologies.
