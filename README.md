# Involuntary Jailbreak

This is the official implementation for [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246).

Implementing this method using APIs is quite simple and can be done in just a few steps. 
In this version, we simply share the `meta_prompt` in `prompt_template/meta_prompts.py` for responsible reasons. 

## Disclaimer
The jailbreaking methods described in this paper are solely intended for responsible AI safety research and the development of more robust, secure AI systems.

### We aim to:
- Help researchers and developers identify vulnerabilities and strengthen defenses.
- Enable the AI community to proactively mitigate risks before malicious exploitation.
- Foster responsible innovation in AI security.

### We urge all readers and users to:
- Any misuse for harmful, unethical, or unauthorized purposes is strictly prohibited.
- Adhere to ethical guidelines and respect platform terms of service.
- Use this knowledge responsibly to advance safe and beneficial AI technologies.

## Citation
We appreciate your acknowledgement to our work by citing:
```bibtex
@misc{involuntary,
      title={Involuntary Jailbreak}, 
      author={Yangyang Guo and Yangyan Li and Mohan Kankanhalli},
      year={2025},
      eprint={2508.13246},
      archivePrefix={arXiv},
}
```

## Acknowledgement
We appreciate [together.ai](https://www.together.ai/) for providing us with the API access to open-sourced models.
For other proprietary LLMs such as OpenAI GPT 4.1 and Grok 4, please visit respective API website for more information.
